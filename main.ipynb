{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber \n",
    "import fitz \n",
    "import os \n",
    "import re \n",
    "import json \n",
    "import pandas as pd\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nlp\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the code with the files in PDF folder in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = 'PDF files/'\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function for Extraction , Preprocessing , tokenizing and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Extraction\n",
    "def extract_text (pdf_path ):\n",
    "    text = ''\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "#Cleaning and removing noise\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+',' ',text) #Removing extra Spaces and new lines\n",
    "    text = re.sub(r'[^a-zA-Z0-9.?! ]', '', text) # Keep sentence-ending punctuation (., ?, !)\n",
    "    return text.lower().strip()\n",
    "\n",
    "#tokenizing into sentences \n",
    "def tokenize_text(text):\n",
    "    return [s.strip() for s in sent_tokenize(text) if s.strip()]\n",
    "\n",
    "#removing stop words in each sentence\n",
    "def removing_stopwords(sentences):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)  \n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]  \n",
    "        filtered_sentences.append(' '.join(filtered_words))  \n",
    "\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing all availabe uploaded PDF files and the user will input the number of file he wants \n",
    "### Note : will be changed later so the user can upload his own pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nðŸ“‚ Available PDFs:')\n",
    "for i, pdf in enumerate(pdf_files,1):\n",
    "    print(f'{i} : {pdf}')\n",
    "choice = int(input(\"\\nEnter the number of the PDF you want to extract text from: \")) - 1\n",
    "if 0 <= choice < len(pdf_files):\n",
    "    PDF_FILE = pdf_files[choice]\n",
    "    PDF_PATH = os.path.join(pdf_dir, PDF_FILE)\n",
    "    print(f\"\\nâœ… Extracting text from: {PDF_FILE}\")\n",
    "    text = extract_text(PDF_PATH)\n",
    "    print(f'\\n {text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(text)\n",
    "tokenized_text = tokenize_text(clean_text)\n",
    "filtered_text = ' '.join(removing_stopwords(tokenized_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(filtered_text)\n",
    "\n",
    "total_words = len(words)\n",
    "unique_words = len(set(words))\n",
    "total_sent = len(tokenized_text)\n",
    "print(\"\\nðŸ“Š Basic Text Statistics:\")\n",
    "print(f\"ðŸ”¹ Total Words: {total_words}\")\n",
    "print(f\"ðŸ”¹ Unique Words: {unique_words}\")\n",
    "print(f\"ðŸ”¹ Total Sentences: {total_sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent 20 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in words if word.isalnum()]  \n",
    "words_count = Counter(words)\n",
    "most_common_words = words_count.most_common(20)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=[word for word, _ in most_common_words], y=[count for _, count in most_common_words])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 20 Most Common Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(filtered_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving extracted text into .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data,filename):\n",
    "    if not os.path.exists('Extracted text'):\n",
    "        os.makedirs('Extracted text')\n",
    "    \n",
    "    file_path = os.path.join('Extracted text', filename)  \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"âœ… Filtered text saved as {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(filtered_text, 'Art of War')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
