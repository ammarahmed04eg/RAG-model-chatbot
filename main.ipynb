{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber \n",
    "import fitz \n",
    "import os \n",
    "import re \n",
    "import json \n",
    "import pandas as pd\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nlp\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the code with the files in PDF folder in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = 'PDF files/'\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function for Extraction , Preprocessing , tokenizing and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Extraction\n",
    "def extract_text (pdf_path ):\n",
    "    text = ''\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "#Cleaning and removing noise\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces and new lines, but keep punctuation like periods and question marks\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.?! ]', '', text)  # Keep important punctuation\n",
    "    return text.lower().strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,  # Max characters per chunk\n",
    "        chunk_overlap=chunk_overlap,  # Overlap for context continuity\n",
    "        length_function=len,  # Measure length by characters\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Hierarchical splitting\n",
    "    )\n",
    "    return text_splitter.split_text(text) \n",
    "\n",
    "# Tokenizing the text into sentences\n",
    "def tokenize_text(text):\n",
    "    return [s.strip() for s in sent_tokenize(text) if s.strip()]\n",
    "\n",
    "\n",
    "#removing stop words in each sentence\n",
    "def removing_stopwords(sentences):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)  \n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]  \n",
    "        filtered_sentences.append(' '.join(filtered_words))  \n",
    "\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing all availabe uploaded PDF files and the user will input the number of file he wants \n",
    "### Note : will be changed later so the user can upload his own pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nðŸ“‚ Available PDFs:')\n",
    "for i, pdf in enumerate(pdf_files,1):\n",
    "    print(f'{i} : {pdf}')\n",
    "choice = int(input(\"\\nEnter the number of the PDF you want to extract text from: \")) - 1\n",
    "if 0 <= choice < len(pdf_files):\n",
    "    PDF_FILE = pdf_files[choice]\n",
    "    PDF_PATH = os.path.join(pdf_dir, PDF_FILE)\n",
    "    print(f\"\\nâœ… Extracting text from: {PDF_FILE}\")\n",
    "    text = extract_text(PDF_PATH)\n",
    "    print(f'\\n {text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(text)\n",
    "tokenized_text = tokenize_text(cleaned_text)\n",
    "filtered_text = ' '.join(removing_stopwords(tokenized_text))\n",
    "chunks = chunk_text(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(filtered_text)\n",
    "\n",
    "total_words = len(words)\n",
    "unique_words = len(set(words))\n",
    "total_sent = len(tokenized_text)\n",
    "print(\"\\nðŸ“Š Basic Text Statistics:\")\n",
    "print(f\"ðŸ”¹ Total Words: {total_words}\")\n",
    "print(f\"ðŸ”¹ Unique Words: {unique_words}\")\n",
    "print(f\"ðŸ”¹ Total Sentences: {total_sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent 20 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in words if word.isalnum()]  \n",
    "words_count = Counter(words)\n",
    "most_common_words = words_count.most_common(20)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=[word for word, _ in most_common_words], y=[count for _, count in most_common_words])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 20 Most Common Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(filtered_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving extracted text into .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data,filename):\n",
    "    if not os.path.exists('Extracted text'):\n",
    "        os.makedirs('Extracted text')\n",
    "    \n",
    "    file_path = os.path.join('Extracted text', filename)  \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"âœ… Filtered text saved as {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(filtered_text, 'Art of War') #Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing the needed libraries SentenceTransformer and faiss ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the embedding model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks, model_name='all-MiniLM-L6-v2', batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate embeddings for text chunks using SentenceTransformer.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = []\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings), model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating FAISS Index\n",
    "\n",
    "After generating embeddings for the sentences, we use FAISS (Facebook AI Similarity Search) to create an index that stores these embeddings. The index allows for fast similarity searches using the L2 (Euclidean) distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_faiss_index(embeddings, index_file='faiss_index.bin'):\n",
    "    \"\"\"\n",
    "    Set up a FAISS index for storing embeddings and save it to disk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity search\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, index_file)\n",
    "        print(f\"âœ… FAISS index saved to {index_file}\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up FAISS index: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_file = f'faiss_index_{PDF_FILE}.bin'\n",
    "embeddings, model = generate_embeddings(chunks)\n",
    "faiss_index = setup_faiss_index(embeddings, index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the Search\n",
    "\n",
    "For a given query, we generate its embedding using the SentenceTransformer model and search for the most similar sentences in the FAISS index. The top-k most relevant sentences are then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_faiss(query, chunks, index, model, top_k=5, threshold=1.0):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_embedding, top_k)  # D: distances, I: indices\n",
    "    results = [(chunks[i], D[0][j]) for j, i in enumerate(I[0]) if D[0][j] <= threshold]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Preprocessing\n",
    "\n",
    "The user's query is processed in the same way as the document text: cleaned, tokenized, and stopwords are removed. This ensures that the query is in the same format as the sentences stored in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    # Clean and remove stop words from the query, similar to your text preprocessing\n",
    "    cleaned_query = clean_text(query)\n",
    "    tokenized_query = tokenize_text(cleaned_query)\n",
    "    filtered_query = ' '.join(removing_stopwords(tokenized_query))\n",
    "    return filtered_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Query Input\n",
    "\n",
    "The user enters a search query, which is then processed and used to retrieve the most relevant sentences from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = input(\"\\nEnter a test query (e.g., 'What is the art of war about?'): \")\n",
    "print(\"\\nðŸ” Top relevant chunks:\")\n",
    "results = search_similar_faiss(test_query, chunks, faiss_index, model, top_k=5)\n",
    "for i, (chunk, distance) in enumerate(results, 1):\n",
    "    print(f\"Chunk {i}: {chunk[:100]}... (L2 Distance: {distance:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
